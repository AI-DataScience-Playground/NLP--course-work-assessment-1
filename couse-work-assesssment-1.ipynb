{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6247,
     "status": "ok",
     "timestamp": 1742139454724,
     "user": {
      "displayName": "Hassan",
      "userId": "14088642748927287081"
     },
     "user_tz": -300
    },
    "id": "0otqWCYQkh47"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy pandas matplotlib seaborn gensim scikit-learn tqdm nltk scipy joblib gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1742139454736,
     "user": {
      "displayName": "Hassan",
      "userId": "14088642748927287081"
     },
     "user_tz": -300
    },
    "id": "RRkBeff3JlYR",
    "outputId": "05d56802-29ba-4c7f-894f-7ae513075a7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueCl8lTAmKcA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qf2VHJfHMNzUKpy7KxkadqLb8zWAqxDD\n",
      "To: c:\\Users\\DELL\\Desktop\\Projects\\NLP--course-work-assessment-1\\cyberbullying_dataset.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.08M/1.08M [00:00<00:00, 9.22MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_id = \"1qf2VHJfHMNzUKpy7KxkadqLb8zWAqxDD\"\n",
    "output_path = \"./cyberbullying_dataset.zip\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False, fuzzy=True)\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tv37qXZPmS9J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: ./\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"./cyberbullying_dataset.zip\"\n",
    "extract_folder = \"./\"\n",
    "\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(f\"File extracted to: {extract_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aLBkDwtRkjq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>gender</th>\n",
       "      <th>sexual orientation</th>\n",
       "      <th>miscellaneous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 u0 lmao wow fuck you too ðŸ˜‚ ðŸ˜‚</td>\n",
       "      <td>normal</td>\n",
       "      <td>No_race</td>\n",
       "      <td>Nonreligious</td>\n",
       "      <td>No_gender</td>\n",
       "      <td>No_orientation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0 th floor maybe wow cnn with the fakenews t...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>No_race</td>\n",
       "      <td>Nonreligious</td>\n",
       "      <td>No_gender</td>\n",
       "      <td>No_orientation</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 0 yrs &lt;number&gt; white women raped by niggers ...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>African</td>\n",
       "      <td>Nonreligious</td>\n",
       "      <td>No_gender</td>\n",
       "      <td>No_orientation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 2 h ago ching chong accepted your friend req...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Nonreligious</td>\n",
       "      <td>No_gender</td>\n",
       "      <td>No_orientation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 8 th century mayhem and lawlessness had noth...</td>\n",
       "      <td>normal</td>\n",
       "      <td>No_race</td>\n",
       "      <td>Nonreligious</td>\n",
       "      <td>No_gender</td>\n",
       "      <td>No_orientation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment       label     race  \\\n",
       "0                     0 u0 lmao wow fuck you too ðŸ˜‚ ðŸ˜‚      normal  No_race   \n",
       "1  1 0 th floor maybe wow cnn with the fakenews t...   offensive  No_race   \n",
       "2  1 0 yrs <number> white women raped by niggers ...  hatespeech  African   \n",
       "3  1 2 h ago ching chong accepted your friend req...   offensive    Asian   \n",
       "4  1 8 th century mayhem and lawlessness had noth...      normal  No_race   \n",
       "\n",
       "       religion     gender sexual orientation miscellaneous  \n",
       "0  Nonreligious  No_gender     No_orientation           NaN  \n",
       "1  Nonreligious  No_gender     No_orientation         Other  \n",
       "2  Nonreligious  No_gender     No_orientation           NaN  \n",
       "3  Nonreligious  No_gender     No_orientation           NaN  \n",
       "4  Nonreligious  No_gender     No_orientation           NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_df = pd.read_csv('./cyberbullying_dataset.csv')\n",
    "imported_df.columns = imported_df.columns.str.lower()\n",
    "imported_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z67gChBcTsXU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20109, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKCvAh5TUr7D"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uZUYsdbT6HX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20109 entries, 0 to 20108\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   comment             20109 non-null  object\n",
      " 1   label               20109 non-null  object\n",
      " 2   race                20109 non-null  object\n",
      " 3   religion            20109 non-null  object\n",
      " 4   gender              20109 non-null  object\n",
      " 5   sexual orientation  20109 non-null  object\n",
      " 6   miscellaneous       3533 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "imported_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4jS23DvU_MG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'label', 'race', 'religion', 'gender', 'sexual orientation'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the 'miscellaneous' column as it has a lot of null values\n",
    "new_df = imported_df.drop(columns=['miscellaneous'])\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSWhSniTVehL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal        7818\n",
       "hatespeech    6484\n",
       "offensive     5807\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if labels are balanced or imbalanced\n",
    "new_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSSJGx7K2_0F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "offensive    12291\n",
       "normal        7818\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['label'] = new_df['label'].replace({'hatespeech': 'offensive'})\n",
    "new_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEw9tE2daAup"
   },
   "source": [
    "# **EDA =>  Univariant Analysis**\n",
    "\n",
    "**Conclusions**\n",
    "- Data is slightly imbalanced with two labels Normal and Offensive having ratio of 38.9%, 61.1% repectively\n",
    "- Gender, Religion and Sexual Orientation columns have more than 75% cells with unspecified information\n",
    "- Race column might have correlation with target labels as the ratio of unspecific data is less than 70% or the ratio of specific data is more than 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAquL8bMpfA5"
   },
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05UYHl7UpwO1"
   },
   "outputs": [],
   "source": [
    "new_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WuRGIwSpxhM"
   },
   "outputs": [],
   "source": [
    "new_df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQNaKnVGpzjb"
   },
   "outputs": [],
   "source": [
    "new_df['religion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8DQQJrYp4Kh"
   },
   "outputs": [],
   "source": [
    "new_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Qr3ZBap-VV"
   },
   "outputs": [],
   "source": [
    "new_df['sexual orientation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN6sVJcrn3qG"
   },
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Labels\": new_df['label'].value_counts(),\n",
    "    \"Gender\": new_df['gender'].value_counts(),\n",
    "    \"Race\": new_df['race'].value_counts(),\n",
    "    \"Religion\": new_df['religion'].value_counts(),\n",
    "    \"Sexual Orientation\": new_df['sexual orientation'].value_counts()\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(14, 25))\n",
    "\n",
    "for i, (category, data) in enumerate(categories.items()):\n",
    "    data.plot(kind='bar', ax=ax[i, 0])\n",
    "    ax[i, 0].set_title(f\"{category} Distribution\")\n",
    "    ax[i, 0].set_xlabel(\"\")\n",
    "    ax[i, 0].tick_params(axis='x', rotation=0)\n",
    "    ax[i, 1].pie(data, labels=data.index, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIbB6c5mufU6"
   },
   "source": [
    "# **EDA => Bivariant Analysis**\n",
    "\n",
    "- We can see clear correlation of different columns with target label\n",
    "- In gender the ratio of hate speech used by Women is much greater as compared to Men\n",
    "- In religion column, Jewish, Hindu, Islam has more percentage of offensive comments as compared to normal comments\n",
    "- Ratio of offensive comments given by Indians, Arabs and Africans is higher\n",
    "- In orientation, Bi-sexuals have higher hate speech ratio as compared to normal comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nwuRW3Zvend"
   },
   "outputs": [],
   "source": [
    "gender_label_relation = pd.crosstab(new_df['label'], new_df['gender'], normalize='columns') * 100\n",
    "gender_label_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEKhbl-Lvwcd"
   },
   "outputs": [],
   "source": [
    "religion_label_relation = pd.crosstab(new_df['label'], new_df['religion'], normalize='columns') * 100\n",
    "religion_label_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl5dxI0Av8NF"
   },
   "outputs": [],
   "source": [
    "race_label_relation = pd.crosstab(new_df['label'], new_df['race'], normalize='columns') * 100\n",
    "race_label_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9shCHirwFeI"
   },
   "outputs": [],
   "source": [
    "orientation_label_relation = pd.crosstab(new_df['label'], new_df['sexual orientation'], normalize='columns') * 100\n",
    "orientation_label_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIFaz0MbxVC3"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "sns.heatmap(gender_label_relation, ax=ax[0, 0], cmap='Blues')\n",
    "ax[0, 0].set_title(\"Gender vs Label\")\n",
    "\n",
    "sns.heatmap(race_label_relation, ax=ax[0, 1], cmap='Blues')\n",
    "ax[0, 1].set_title(\"Race vs Label\")\n",
    "\n",
    "sns.heatmap(religion_label_relation, ax=ax[1, 0], cmap='Blues')\n",
    "ax[1, 0].set_title(\"Religion vs Label\")\n",
    "\n",
    "sns.heatmap(orientation_label_relation, ax=ax[1, 1], cmap='Blues')\n",
    "ax[1, 1].set_title(\"Sexual Orientation vs Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQlL-Vpv1492"
   },
   "source": [
    "# **Preprocessing and Feature Engineering**\n",
    "- basic preprocesing\n",
    "- tokenization\n",
    "- combine all columns into 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmqkYkEe5jlU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing muhammad hassaan maqbool conducting test\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "print(preprocess_text(\"it's just testing. I am Muhammad Hassaan Maqbool conducting the test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOrZTprZ-jco"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u lmao wow fuck norace nonreligious nogender n...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>th floor maybe wow cnn fakenews body count goi...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yr number white woman raped nigger number negr...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h ago ching chong accepted friend request asia...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>th century mayhem lawlessness nothing loud mou...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment      label\n",
       "0  u lmao wow fuck norace nonreligious nogender n...     normal\n",
       "1  th floor maybe wow cnn fakenews body count goi...  offensive\n",
       "2  yr number white woman raped nigger number negr...  offensive\n",
       "3  h ago ching chong accepted friend request asia...  offensive\n",
       "4  th century mayhem lawlessness nothing loud mou...     normal"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = new_df.copy()\n",
    "processed_df['comment'] = processed_df['comment'] + ' ' + processed_df['race'] + ' ' + processed_df['religion'] + ' ' + processed_df['gender'] + ' ' + processed_df['sexual orientation']\n",
    "processed_df = processed_df[['comment', 'label']]\n",
    "processed_df['comment'] = processed_df['comment'].apply(preprocess_text)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9QfzzcVBR1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    12291\n",
       "0     7818\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['label'] = processed_df['label'].map({'normal': 0, 'offensive': 1})\n",
    "processed_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1_ERU6SFkLh"
   },
   "source": [
    "# **Modeling and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHFWL78z1hp8"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlUKd037k-xx"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, just_evaluate = False):\n",
    "    if(just_evaluate == False):\n",
    "      model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return accuracy, conf_matrix, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgbwXJvZ3-Nv"
   },
   "outputs": [],
   "source": [
    "def tune_model_random_search(model, param_dist, x_train, y_train, n_iter=10, cv=3, n_jobs=-1, verbose=2, random_state=42):\n",
    "    random_search = RandomizedSearchCV(estimator=model,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter,\n",
    "                                       cv=cv,\n",
    "                                       verbose=verbose,\n",
    "                                       random_state=random_state,\n",
    "                                       n_jobs=n_jobs)\n",
    "    random_search.fit(x_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RakLbHOdS-oW"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_all_models(x_train, x_test, y_train, y_test):\n",
    "  models = {\n",
    "  'rfc' : RandomForestClassifier(n_jobs=-1),\n",
    "  'lg' : LogisticRegression(n_jobs=-1),\n",
    "  'svc' : SVC(),\n",
    "  'gnb' : GaussianNB(),\n",
    "  'xgb' : XGBClassifier(n_jobs=-1)\n",
    "  }\n",
    "\n",
    "  for model_name, model in tqdm(models.items(), desc=\"Training models\", total=len(models)):\n",
    "      print(f\"Evaluating model: {model.__class__.__name__}\")\n",
    "      if model_name == 'gnb':\n",
    "            x_train_dense = x_train.toarray()\n",
    "            x_test_dense = x_test.toarray()\n",
    "            train_and_evaluate(model, x_train_dense, x_test_dense, y_train, y_test, False)\n",
    "      else:\n",
    "          train_and_evaluate(model, x_train, x_test, y_train, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECx7W_aR125H"
   },
   "source": [
    "# **Using TF-IDF Vectorizer**\n",
    "**Conclusions**\n",
    "- XGBoost, RandomForest and SVC performed really well with accuracy and precision between 84% to 87%\n",
    "- Performance of Naive Bayes was poor with accuracy and percision of approximately 50% and 69% respectively\n",
    "- Considering their initial performance, tried Hyperparameter tuning on XGBoost, Random Forest and SVC. Multiple iterations were tried with randomized hyperparameters to find the best combination, but couldn't find anymore improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYnQhqGKBtgI"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "x, y = tfidf.fit_transform(processed_df['comment']), processed_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve-VHl_8TXkq"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSd1kC3JjNu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-9AJQlyQtU5"
   },
   "source": [
    "**Hyperparameter Tuning**\n",
    "Performing tuning on the following algorigthms\n",
    "- Random Forest Classifier\n",
    "- XGBoost\n",
    "- SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A9OnUvHTyZs"
   },
   "source": [
    "**Random Forest Hyperparameter Tuning**\n",
    "- tried multiple iteration on random parameters but didn't see any improvement as compared to default one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR9KbdQGSMfl"
   },
   "outputs": [],
   "source": [
    "param_rfc = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],\n",
    "    'max_features': [None, 'sqrt'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "best_model_rfc, best_params_rfc = tune_model_random_search(RandomForestClassifier(), param_rfc, x_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", best_params_rfc)\n",
    "\n",
    "train_and_evaluate(best_model_rfc, x_train, x_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTHxSRVheTtN"
   },
   "source": [
    "**XGBoost Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgZRIeTCqLLM"
   },
   "outputs": [],
   "source": [
    "param_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "best_model_xgb, best_params_xgb = tune_model_random_search(XGBClassifier(), param_xgb, x_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", best_params_xgb)\n",
    "\n",
    "train_and_evaluate(best_model_xgb, x_train, x_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlWdNNqLqw7K"
   },
   "source": [
    "**SVC Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkwRtZQ8rEnR"
   },
   "outputs": [],
   "source": [
    "param_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "best_model_svc, best_params_svc = tune_model_random_search(SVC(), param_svc, x_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", best_params_svc)\n",
    "\n",
    "train_and_evaluate(best_model_svc, x_train, x_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRU6_E9qUDfE"
   },
   "source": [
    "# **Using Word2Vec**\n",
    "**Conclusion**\n",
    "- Couldn't find any improvements as compared to TF-IDF approach\n",
    "- Maximum accuracy and precision achieved are 80% and 83% respectively, meanwhile for TF-IDF it accuracy and precisions were more than 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7oNzslSVO6D"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JftWFa7q9CZo"
   },
   "outputs": [],
   "source": [
    "def get_comment_vector_from_words(comment, model):\n",
    "  words = [word for word in comment if word in model.wv]\n",
    "  if len(words) == 0:\n",
    "    return np.zeros(model.vector_size)\n",
    "  return np.mean([model.wv[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPLjr12H5Q1t"
   },
   "outputs": [],
   "source": [
    "word2vec_df = processed_df[['comment', 'label']]\n",
    "word2vec_df['comment'] = word2vec_df['comment'].apply(word_tokenize)\n",
    "word2vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv12t74f59eO"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = word2vec_df['comment'], window=5, min_count=1, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3qGfmXMBXAQ"
   },
   "outputs": [],
   "source": [
    "word2vec_df['wv_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_words(x, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbkBYa8gC9YF"
   },
   "outputs": [],
   "source": [
    "x, y = csr_matrix(np.vstack(word2vec_df['wv_vectors'])), word2vec_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNz-_PW3ENPc"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6liNX9C3GNK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oPo58817PET"
   },
   "source": [
    "# **GloVe (Twitter Pretrained Model)**\n",
    "**Conclusion**\n",
    "- Couldn't see any more improvements even after using both Twitter and Google News Corpus Pretrained Models.\n",
    "- TF-IDF out performed customer Word2Vec and GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tKAcw53I0R4"
   },
   "outputs": [],
   "source": [
    "def get_comment_vector_from_glove(comment, embeddings):\n",
    "  words = [word for word in comment if word in embeddings]\n",
    "  if len(words) == 0:\n",
    "    return np.zeros(model.vector_size)\n",
    "  return np.mean([embeddings[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8VIWIG8FyD9"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alWxOT_0mLsH"
   },
   "source": [
    "**Twitter 27B Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9N379euhECpH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1tH_jyhTIPmiomCKeapvSwDjh6aXsFBcP\n",
      "From (redirected): https://drive.google.com/uc?id=1tH_jyhTIPmiomCKeapvSwDjh6aXsFBcP&confirm=t&uuid=f008cd69-47ca-4b90-8fab-a688eabc43c5\n",
      "To: c:\\Users\\DELL\\Desktop\\Projects\\NLP--course-work-assessment-1\\glove.twitter.27B.100d.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411M/411M [00:10<00:00, 39.1MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_id = \"1tH_jyhTIPmiomCKeapvSwDjh6aXsFBcP\"\n",
    "output_path = \"./glove.twitter.27B.100d.zip\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False, fuzzy=True)\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewJ1bE88Ejqs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: ./glove_twitter\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"./glove.twitter.27B.100d.zip\"\n",
    "extract_folder = \"./glove_twitter\"\n",
    "\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(f\"File extracted to: {extract_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwWEjZXkF3YX"
   },
   "outputs": [],
   "source": [
    "twitter_glove_path = \"glove_twitter/glove.twitter.27B.100d.txt\"\n",
    "twitter_glove_embeddings = load_glove_embeddings(twitter_glove_path)\n",
    "\n",
    "print(f\"Loaded {len(twitter_glove_embeddings)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk3BHAJbGFH7"
   },
   "outputs": [],
   "source": [
    "word2vec_df['glove_twitter_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_glove(x, twitter_glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeFnLya8JBNy"
   },
   "outputs": [],
   "source": [
    "x, y = csr_matrix(np.vstack(word2vec_df['glove_twitter_vectors'])), word2vec_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgYnv0kkJRbZ"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE19TWAkmWKW"
   },
   "source": [
    "**Google 300B Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-GYFrxJYpfi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1elKVMmcGpvNZJvxwg9enc2B_lOSCjP1t\n",
      "From (redirected): https://drive.google.com/uc?id=1elKVMmcGpvNZJvxwg9enc2B_lOSCjP1t&confirm=t&uuid=766b395b-109c-4960-b856-b76e6661b151\n",
      "To: c:\\Users\\DELL\\Desktop\\Projects\\NLP--course-work-assessment-1\\GoogleNews-vectors-negative300.bin.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:58<00:00, 30.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_id = \"1elKVMmcGpvNZJvxwg9enc2B_lOSCjP1t\"\n",
    "output_path = \"./GoogleNews-vectors-negative300.bin.zip\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CYiSKwfaMvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: ./\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"./GoogleNews-vectors-negative300.bin.zip\"\n",
    "extract_folder = \"./\"\n",
    "\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(f\"File extracted to: {extract_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7AbV5_MzISW"
   },
   "outputs": [],
   "source": [
    "google_new_model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "google_news_model = gensim.models.KeyedVectors.load_word2vec_format(google_new_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5i9YDDzzkd_"
   },
   "outputs": [],
   "source": [
    "google_news_model.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feQkVNe1zpxW"
   },
   "outputs": [],
   "source": [
    "word2vec_df['glove_google_news_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_glove(x, google_news_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYv1izN60eEi"
   },
   "outputs": [],
   "source": [
    "x, y = csr_matrix(np.vstack(word2vec_df['glove_twitter_vectors'])), word2vec_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4X42F5Q0iOt"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlBkCRCgeYgYAGQeh5E36t",
   "collapsed_sections": [
    "oEw9tE2daAup",
    "ZIbB6c5mufU6",
    "RQlL-Vpv1492",
    "k1_ERU6SFkLh",
    "ECx7W_aR125H"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
