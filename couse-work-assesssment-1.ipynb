{"cells":[{"cell_type":"code","source":["!pip install --upgrade numpy pandas matplotlib seaborn gensim scikit-learn tqdm nltk scipy joblib gdown --quiet"],"metadata":{"id":"0otqWCYQkh47","executionInfo":{"status":"ok","timestamp":1742139454724,"user_tz":-300,"elapsed":6247,"user":{"displayName":"Hassan","userId":"14088642748927287081"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1742139454736,"user":{"displayName":"Hassan","userId":"14088642748927287081"},"user_tz":-300},"id":"RRkBeff3JlYR","outputId":"05d56802-29ba-4c7f-894f-7ae513075a7c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":67}],"source":["import zipfile\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import re\n","import string\n","from tqdm import tqdm\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","source":["file_id = \"1qf2VHJfHMNzUKpy7KxkadqLb8zWAqxDD\"\n","output_path = \"/content/\"\n","gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n","print(\"Download complete!\")\n"],"metadata":{"id":"ueCl8lTAmKcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip cyberbullying_dataset.zip -d ./"],"metadata":{"id":"Tv37qXZPmS9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aLBkDwtRkjq"},"outputs":[],"source":["imported_df = pd.read_csv('/content/cyberbullying_dataset.csv')\n","imported_df.columns = imported_df.columns.str.lower()\n","imported_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z67gChBcTsXU"},"outputs":[],"source":["imported_df.shape"]},{"cell_type":"markdown","metadata":{"id":"ZKCvAh5TUr7D"},"source":["# **Exploratory Data Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uZUYsdbT6HX"},"outputs":[],"source":["imported_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4jS23DvU_MG"},"outputs":[],"source":["# dropping the 'miscellaneous' column as it has a lot of null values\n","new_df = imported_df.drop(columns=['miscellaneous'])\n","new_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSWhSniTVehL"},"outputs":[],"source":["# check if labels are balanced or imbalanced\n","new_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSSJGx7K2_0F"},"outputs":[],"source":["new_df['label'] = new_df['label'].replace({'hatespeech': 'offensive'})\n","new_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"oEw9tE2daAup"},"source":["# **EDA =>  Univariant Analysis**\n","\n","**Conclusions**\n","- Data is slightly imbalanced with two labels Normal and Offensive having ratio of 38.9%, 61.1% repectively\n","- Gender, Religion and Sexual Orientation columns have more than 75% cells with unspecified information\n","- Race column might have correlation with target labels as the ratio of unspecific data is less than 70% or the ratio of specific data is more than 30%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAquL8bMpfA5"},"outputs":[],"source":["new_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05UYHl7UpwO1"},"outputs":[],"source":["new_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WuRGIwSpxhM"},"outputs":[],"source":["new_df['race'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQNaKnVGpzjb"},"outputs":[],"source":["new_df['religion'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8DQQJrYp4Kh"},"outputs":[],"source":["new_df['gender'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7Qr3ZBap-VV"},"outputs":[],"source":["new_df['sexual orientation'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sN6sVJcrn3qG"},"outputs":[],"source":["categories = {\n","    \"Labels\": new_df['label'].value_counts(),\n","    \"Gender\": new_df['gender'].value_counts(),\n","    \"Race\": new_df['race'].value_counts(),\n","    \"Religion\": new_df['religion'].value_counts(),\n","    \"Sexual Orientation\": new_df['sexual orientation'].value_counts()\n","}\n","\n","fig, ax = plt.subplots(5, 2, figsize=(14, 25))\n","\n","for i, (category, data) in enumerate(categories.items()):\n","    data.plot(kind='bar', ax=ax[i, 0])\n","    ax[i, 0].set_title(f\"{category} Distribution\")\n","    ax[i, 0].set_xlabel(\"\")\n","    ax[i, 0].tick_params(axis='x', rotation=0)\n","    ax[i, 1].pie(data, labels=data.index, autopct='%1.1f%%', startangle=90)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZIbB6c5mufU6"},"source":["# **EDA => Bivariant Analysis**\n","\n","- We can see clear correlation of different columns with target label\n","- In gender the ratio of hate speech used by Women is much greater as compared to Men\n","- In religion column, Jewish, Hindu, Islam has more percentage of offensive comments as compared to normal comments\n","- Ratio of offensive comments given by Indians, Arabs and Africans is higher\n","- In orientation, Bi-sexuals have higher hate speech ratio as compared to normal comments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nwuRW3Zvend"},"outputs":[],"source":["gender_label_relation = pd.crosstab(new_df['label'], new_df['gender'], normalize='columns') * 100\n","gender_label_relation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEKhbl-Lvwcd"},"outputs":[],"source":["religion_label_relation = pd.crosstab(new_df['label'], new_df['religion'], normalize='columns') * 100\n","religion_label_relation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xl5dxI0Av8NF"},"outputs":[],"source":["race_label_relation = pd.crosstab(new_df['label'], new_df['race'], normalize='columns') * 100\n","race_label_relation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9shCHirwFeI"},"outputs":[],"source":["orientation_label_relation = pd.crosstab(new_df['label'], new_df['sexual orientation'], normalize='columns') * 100\n","orientation_label_relation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIFaz0MbxVC3"},"outputs":[],"source":["fig, ax = plt.subplots(2, 2, figsize=(14, 12))\n","\n","sns.heatmap(gender_label_relation, ax=ax[0, 0], cmap='Blues')\n","ax[0, 0].set_title(\"Gender vs Label\")\n","\n","sns.heatmap(race_label_relation, ax=ax[0, 1], cmap='Blues')\n","ax[0, 1].set_title(\"Race vs Label\")\n","\n","sns.heatmap(religion_label_relation, ax=ax[1, 0], cmap='Blues')\n","ax[1, 0].set_title(\"Religion vs Label\")\n","\n","sns.heatmap(orientation_label_relation, ax=ax[1, 1], cmap='Blues')\n","ax[1, 1].set_title(\"Sexual Orientation vs Label\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RQlL-Vpv1492"},"source":["# **Preprocessing and Feature Engineering**\n","- basic preprocesing\n","- tokenization\n","- combine all columns into 1 column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmqkYkEe5jlU"},"outputs":[],"source":["stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    words = word_tokenize(text)\n","    words = [word for word in words if word not in stop_words]\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    return ' '.join(words)\n","\n","\n","print(preprocess_text(\"it's just testing. I am Muhammad Hassaan Maqbool conducting the test\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOrZTprZ-jco"},"outputs":[],"source":["processed_df = new_df.copy()\n","processed_df['comment'] = processed_df['comment'] + ' ' + processed_df['race'] + ' ' + processed_df['religion'] + ' ' + processed_df['gender'] + ' ' + processed_df['sexual orientation']\n","processed_df = processed_df[['comment', 'label']]\n","processed_df['comment'] = processed_df['comment'].apply(preprocess_text)\n","processed_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9QfzzcVBR1d"},"outputs":[],"source":["processed_df['label'] = processed_df['label'].map({'normal': 0, 'offensive': 1})\n","processed_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"k1_ERU6SFkLh"},"source":["# **Modeling and Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHFWL78z1hp8"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlUKd037k-xx"},"outputs":[],"source":["def train_and_evaluate(model, X_train, X_test, y_train, y_test, just_evaluate = False):\n","    if(just_evaluate == False):\n","      model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","\n","    print(f\"Model: {model.__class__.__name__}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n","    print(\"-\" * 50)\n","\n","    return accuracy, conf_matrix, precision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgbwXJvZ3-Nv"},"outputs":[],"source":["def tune_model_random_search(model, param_dist, x_train, y_train, n_iter=10, cv=3, n_jobs=-1, verbose=2, random_state=42):\n","    random_search = RandomizedSearchCV(estimator=model,\n","                                       param_distributions=param_dist,\n","                                       n_iter=n_iter,\n","                                       cv=cv,\n","                                       verbose=verbose,\n","                                       random_state=random_state,\n","                                       n_jobs=n_jobs)\n","    random_search.fit(x_train, y_train)\n","    best_model = random_search.best_estimator_\n","    best_params = random_search.best_params_\n","    return best_model, best_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RakLbHOdS-oW"},"outputs":[],"source":["def train_and_evaluate_all_models(x_train, x_test, y_train, y_test):\n","  models = {\n","  'rfc' : RandomForestClassifier(n_jobs=-1),\n","  'lg' : LogisticRegression(n_jobs=-1),\n","  'svc' : SVC(),\n","  'gnb' : GaussianNB(),\n","  'xgb' : XGBClassifier(n_jobs=-1)\n","  }\n","\n","  for model_name, model in tqdm(models.items(), desc=\"Training models\", total=len(models)):\n","      print(f\"Evaluating model: {model.__class__.__name__}\")\n","      if model_name == 'gnb':\n","            x_train_dense = x_train.toarray()\n","            x_test_dense = x_test.toarray()\n","            train_and_evaluate(model, x_train_dense, x_test_dense, y_train, y_test, False)\n","      else:\n","          train_and_evaluate(model, x_train, x_test, y_train, y_test, False)"]},{"cell_type":"markdown","metadata":{"id":"ECx7W_aR125H"},"source":["# **Using TF-IDF Vectorizer**\n","**Conclusions**\n","- XGBoost, RandomForest and SVC performed really well with accuracy and precision between 84% to 87%\n","- Performance of Naive Bayes was poor with accuracy and percision of approximately 50% and 69% respectively\n","- Considering their initial performance, tried Hyperparameter tuning on XGBoost, Random Forest and SVC. Multiple iterations were tried with randomized hyperparameters to find the best combination, but couldn't find anymore improvements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYnQhqGKBtgI"},"outputs":[],"source":["tfidf = TfidfVectorizer()\n","\n","x, y = tfidf.fit_transform(processed_df['comment']), processed_df['label']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ve-VHl_8TXkq"},"outputs":[],"source":["train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"jjSd1kC3JjNu"},"source":[]},{"cell_type":"markdown","metadata":{"id":"l-9AJQlyQtU5"},"source":["**Hyperparameter Tuning**\n","Performing tuning on the following algorigthms\n","- Random Forest Classifier\n","- XGBoost\n","- SVC"]},{"cell_type":"markdown","metadata":{"id":"7A9OnUvHTyZs"},"source":["**Random Forest Hyperparameter Tuning**\n","- tried multiple iteration on random parameters but didn't see any improvement as compared to default one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JR9KbdQGSMfl"},"outputs":[],"source":["param_rfc = {\n","    'n_estimators': [50, 100, 200, 300, 500],\n","    'max_features': [None, 'sqrt'],\n","    'max_depth': [None, 10, 20, 30, 40, 50],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'bootstrap': [True, False]\n","}\n","\n","best_model_rfc, best_params_rfc = tune_model_random_search(RandomForestClassifier(), param_rfc, x_train, y_train)\n","\n","print(\"Best Parameters:\", best_params_rfc)\n","\n","train_and_evaluate(best_model_rfc, x_train, x_test, y_train, y_test, True)"]},{"cell_type":"markdown","metadata":{"id":"tTHxSRVheTtN"},"source":["**XGBoost Hyperparameter Tuning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgZRIeTCqLLM"},"outputs":[],"source":["param_xgb = {\n","    'n_estimators': [100, 200, 300],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [3, 5, 7],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0],\n","    'gamma': [0, 0.1, 0.3]\n","}\n","\n","best_model_xgb, best_params_xgb = tune_model_random_search(XGBClassifier(), param_xgb, x_train, y_train)\n","\n","print(\"Best Parameters:\", best_params_xgb)\n","\n","train_and_evaluate(best_model_xgb, x_train, x_test, y_train, y_test, True)"]},{"cell_type":"markdown","metadata":{"id":"KlWdNNqLqw7K"},"source":["**SVC Hyperparameter Tuning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkwRtZQ8rEnR"},"outputs":[],"source":["param_svc = {\n","    'C': [0.1, 1, 10, 100],\n","    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n","    'gamma': ['scale', 'auto'],\n","    'degree': [2, 3, 4],\n","    'class_weight': [None, 'balanced']\n","}\n","\n","best_model_svc, best_params_svc = tune_model_random_search(SVC(), param_svc, x_train, y_train)\n","\n","print(\"Best Parameters:\", best_params_svc)\n","\n","train_and_evaluate(best_model_svc, x_train, x_test, y_train, y_test, True)"]},{"cell_type":"markdown","metadata":{"id":"oRU6_E9qUDfE"},"source":["# **Using Word2Vec**\n","**Conclusion**\n","- Couldn't find any improvements as compared to TF-IDF approach\n","- Maximum accuracy and precision achieved are 80% and 83% respectively, meanwhile for TF-IDF it accuracy and precisions were more than 84%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7oNzslSVO6D"},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JftWFa7q9CZo"},"outputs":[],"source":["def get_comment_vector_from_words(comment, model):\n","  words = [word for word in comment if word in model.wv]\n","  if len(words) == 0:\n","    return np.zeros(model.vector_size)\n","  return np.mean([model.wv[word] for word in words], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPLjr12H5Q1t"},"outputs":[],"source":["word2vec_df = processed_df[['comment', 'label']]\n","word2vec_df['comment'] = word2vec_df['comment'].apply(word_tokenize)\n","word2vec_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv12t74f59eO"},"outputs":[],"source":["model = Word2Vec(sentences = word2vec_df['comment'], window=5, min_count=1, workers=4, sg=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3qGfmXMBXAQ"},"outputs":[],"source":["word2vec_df['wv_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_words(x, model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbkBYa8gC9YF"},"outputs":[],"source":["x, y = csr_matrix(np.vstack(word2vec_df['wv_vectors'])), word2vec_df['label']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNz-_PW3ENPc"},"outputs":[],"source":["train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"u6liNX9C3GNK"},"source":[]},{"cell_type":"markdown","metadata":{"id":"0oPo58817PET"},"source":["# **GloVe (Twitter Pretrained Model)**\n","**Conclusion**\n","- Couldn't see any more improvements even after using both Twitter and Google News Corpus Pretrained Models.\n","- TF-IDF out performed customer Word2Vec and GloVe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tKAcw53I0R4"},"outputs":[],"source":["def get_comment_vector_from_glove(comment, embeddings):\n","  words = [word for word in comment if word in embeddings]\n","  if len(words) == 0:\n","    return np.zeros(model.vector_size)\n","  return np.mean([embeddings[word] for word in words], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8VIWIG8FyD9"},"outputs":[],"source":["def load_glove_embeddings(file_path):\n","    embeddings = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = np.array(values[1:], dtype='float32')\n","            embeddings[word] = vector\n","    return embeddings"]},{"cell_type":"markdown","metadata":{"id":"alWxOT_0mLsH"},"source":["**Twitter 27B Corpus**"]},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"],"metadata":{"id":"9N379euhECpH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip glove.twitter.27B.zip -d ./glove_twitter"],"metadata":{"id":"ewJ1bE88Ejqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwWEjZXkF3YX"},"outputs":[],"source":["twitter_glove_path = \"glove_twitter/glove.twitter.27B.100d.txt\"\n","twitter_glove_embeddings = load_glove_embeddings(twitter_glove_path)\n","\n","print(f\"Loaded {len(twitter_glove_embeddings)} word vectors.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bk3BHAJbGFH7"},"outputs":[],"source":["word2vec_df['glove_twitter_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_glove(x, twitter_glove_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeFnLya8JBNy"},"outputs":[],"source":["x, y = csr_matrix(np.vstack(word2vec_df['glove_twitter_vectors'])), word2vec_df['label']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgYnv0kkJRbZ"},"outputs":[],"source":["train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{"id":"hE19TWAkmWKW"},"source":["**Google 300B Corpus**"]},{"cell_type":"code","source":["import gdown\n","\n","file_id = \"1elKVMmcGpvNZJvxwg9enc2B_lOSCjP1t\"\n","output_path = \"/content/GoogleNews-vectors-negative300.bin.zip\"\n","gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n","print(\"Download complete!\")"],"metadata":{"id":"K-GYFrxJYpfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip GoogleNews-vectors-negative300.bin.zip -d ./"],"metadata":{"id":"5CYiSKwfaMvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7AbV5_MzISW"},"outputs":[],"source":["google_new_model_path = \"GoogleNews-vectors-negative300.bin\"\n","google_news_model = gensim.models.KeyedVectors.load_word2vec_format(google_new_model_path, binary=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5i9YDDzzkd_"},"outputs":[],"source":["google_news_model.most_similar(\"king\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feQkVNe1zpxW"},"outputs":[],"source":["word2vec_df['glove_google_news_vectors'] = word2vec_df['comment'].apply(lambda x: get_comment_vector_from_glove(x, google_news_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYv1izN60eEi"},"outputs":[],"source":["x, y = csr_matrix(np.vstack(word2vec_df['glove_twitter_vectors'])), word2vec_df['label']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4X42F5Q0iOt"},"outputs":[],"source":["train_and_evaluate_all_models(x_train, x_test, y_train, y_test)"]}],"metadata":{"colab":{"collapsed_sections":["oEw9tE2daAup","ZIbB6c5mufU6","RQlL-Vpv1492","k1_ERU6SFkLh","ECx7W_aR125H"],"provenance":[],"authorship_tag":"ABX9TyMlBkCRCgeYgYAGQeh5E36t"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}